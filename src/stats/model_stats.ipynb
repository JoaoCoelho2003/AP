{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Text Detection Model Evaluation\n",
    "\n",
    "This notebook evaluates multiple models on a test dataset and compares their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.numpyModels.logistic_regression import LogisticRegression\n",
    "from models.numpyModels.dnn.neuralnet import NeuralNetwork\n",
    "from models.numpyModels.rnn.rnn import RNN\n",
    "from models.numpyModels.rnn.optimizers import AdamOptimizer\n",
    "\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input dataset...\n",
      "Error parsing as comma-separated: Error tokenizing data. C error: Expected 11 fields in line 3, saw 15\n",
      "\n",
      "Loading output dataset...\n",
      "Input Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1-1</td>\n",
       "      <td>The cell cycle, or cell-division cycle, is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1-2</td>\n",
       "      <td>The cell cycle is the process by which a cell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1-3</td>\n",
       "      <td>Photons, in many atomic models in physics, are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1-4</td>\n",
       "      <td>A photon is a fundamental particle of light an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1-5</td>\n",
       "      <td>According to the theory of plate tectonics, Ea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                               Text\n",
       "0  D1-1  The cell cycle, or cell-division cycle, is the...\n",
       "1  D1-2  The cell cycle is the process by which a cell ...\n",
       "2  D1-3  Photons, in many atomic models in physics, are...\n",
       "3  D1-4  A photon is a fundamental particle of light an...\n",
       "4  D1-5  According to the theory of plate tectonics, Ea..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset columns: ['ID', 'Text']\n",
      "\n",
      "Output Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID\\tLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1-1\\tHuman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1-2\\tAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1-3\\tHuman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1-4\\tAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1-5\\tHuman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID\\tLabel\n",
       "0  D1-1\\tHuman\n",
       "1     D1-2\\tAI\n",
       "2  D1-3\\tHuman\n",
       "3     D1-4\\tAI\n",
       "4  D1-5\\tHuman"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dataset columns: ['ID\\tLabel']\n",
      "\n",
      "Using columns - Input: ID=ID, Text=Text\n",
      "Using columns - Output: ID=ID\tLabel, Label=None\n",
      "\n",
      "After renaming:\n",
      "Input dataset columns: ['ID', 'Text']\n",
      "Output dataset columns: ['ID']\n"
     ]
    }
   ],
   "source": [
    "inputs_path = \"../datasets/dataset1_inputs.csv\"\n",
    "outputs_path = \"../datasets/dataset1_outputs.csv\"\n",
    "\n",
    "def load_csv(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing as comma-separated: {str(e)}\")\n",
    "        try:\n",
    "            return pd.read_csv(file_path, sep='\\t')\n",
    "        except Exception as e2:\n",
    "            print(f\"Error parsing as tab-separated: {str(e2)}\")\n",
    "            try:\n",
    "                return pd.read_csv(file_path, encoding='latin1')\n",
    "            except Exception as e3:\n",
    "                print(f\"Error with latin1 encoding: {str(e3)}\")\n",
    "                raise Exception(f\"Failed to load CSV file: {file_path}\")\n",
    "\n",
    "print(\"Loading input dataset...\")\n",
    "inputs_df = load_csv(inputs_path)\n",
    "\n",
    "print(\"Loading output dataset...\")\n",
    "outputs_df = load_csv(outputs_path)\n",
    "\n",
    "print(\"Input Dataset:\")\n",
    "display(inputs_df.head())\n",
    "print(f\"Input dataset columns: {inputs_df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nOutput Dataset:\")\n",
    "display(outputs_df.head())\n",
    "print(f\"Output dataset columns: {outputs_df.columns.tolist()}\")\n",
    "\n",
    "def identify_columns(df):\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    id_col = None\n",
    "    for col in columns:\n",
    "        if col.lower() == 'id' or 'id' in col.lower():\n",
    "            id_col = col\n",
    "            break\n",
    "    \n",
    "    if id_col is None and len(columns) > 0:\n",
    "        id_col = columns[0]\n",
    "        print(f\"No ID column found, using first column: {id_col}\")\n",
    "    \n",
    "    content_col = None\n",
    "    for col in columns:\n",
    "        if col.lower() in ['text', 'content', 'label', 'class']:\n",
    "            content_col = col\n",
    "            break\n",
    "    \n",
    "    if content_col is None and len(columns) > 1:\n",
    "        content_col = columns[1]\n",
    "        print(f\"No content column found, using second column: {content_col}\")\n",
    "    \n",
    "    return id_col, content_col\n",
    "\n",
    "input_id_col, input_text_col = identify_columns(inputs_df)\n",
    "output_id_col, output_label_col = identify_columns(outputs_df)\n",
    "\n",
    "print(f\"\\nUsing columns - Input: ID={input_id_col}, Text={input_text_col}\")\n",
    "print(f\"Using columns - Output: ID={output_id_col}, Label={output_label_col}\")\n",
    "\n",
    "inputs_df = inputs_df.rename(columns={input_id_col: 'ID', input_text_col: 'Text'})\n",
    "outputs_df = outputs_df.rename(columns={output_id_col: 'ID', output_label_col: 'Label'})\n",
    "\n",
    "print(\"\\nAfter renaming:\")\n",
    "print(f\"Input dataset columns: {inputs_df.columns.tolist()}\")\n",
    "print(f\"Output dataset columns: {outputs_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded logistic model\n",
      "Successfully loaded dnn model\n",
      "Successfully loaded rnn model\n"
     ]
    }
   ],
   "source": [
    "class DatasetWrapper:\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "def load_model(model_type):\n",
    "    if model_type == \"dnn\":\n",
    "        model = NeuralNetwork()\n",
    "        model.load(\"../trained_models/numpy/dnn_weights.npz\")\n",
    "        \n",
    "        with open(\"../preprocessed/vectorizer.pkl\", \"rb\") as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "        \n",
    "        return model, vectorizer, None, None\n",
    "    \n",
    "    elif model_type == \"rnn\":\n",
    "        embedding_matrix = np.load(\"../preprocessed/embedding_matrix.npy\")\n",
    "        \n",
    "        model = RNN(n_units=64, embedding_matrix=embedding_matrix)\n",
    "        model.initialize(AdamOptimizer())\n",
    "        model.load(\"../trained_models/numpy/rnn_weights.npz\")\n",
    "        \n",
    "        with open(\"../preprocessed/word_to_idx.pkl\", \"rb\") as f:\n",
    "            word_to_idx = pickle.load(f)\n",
    "        \n",
    "        return model, None, word_to_idx, embedding_matrix\n",
    "    \n",
    "    else:\n",
    "        model = LogisticRegression()\n",
    "        model.load(\"../trained_models/numpy/logistic_weights.npz\")\n",
    "        \n",
    "        with open(\"../preprocessed/vectorizer.pkl\", \"rb\") as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "        \n",
    "        return model, vectorizer, None, None\n",
    "\n",
    "models = {}\n",
    "vectorizers = {}\n",
    "word_to_idxs = {}\n",
    "embedding_matrices = {}\n",
    "\n",
    "for model_type in [\"logistic\", \"dnn\", \"rnn\"]:\n",
    "    try:\n",
    "        model, vectorizer, word_to_idx, embedding_matrix = load_model(model_type)\n",
    "        models[model_type] = model\n",
    "        vectorizers[model_type] = vectorizer\n",
    "        word_to_idxs[model_type] = word_to_idx\n",
    "        embedding_matrices[model_type] = embedding_matrix\n",
    "        print(f\"Successfully loaded {model_type} model\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_type} model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def predict_text(model, vectorizer, word_to_idx, embedding_matrix, text, model_type):\n",
    "    tokens = clean_text(text)\n",
    "    \n",
    "    if model_type == \"rnn\":\n",
    "        max_seq_length = 100\n",
    "        sequence = np.zeros((1, max_seq_length), dtype=int)\n",
    "        \n",
    "        for j, word in enumerate(tokens[:max_seq_length]):\n",
    "            if word in word_to_idx:\n",
    "                sequence[0, j] = word_to_idx[word]\n",
    "        \n",
    "        predictions = model.forward_propagation(sequence)\n",
    "        probability = predictions[0, -1, 0]\n",
    "    \n",
    "    else:\n",
    "        joined_text = \" \".join(tokens)\n",
    "        X_new = vectorizer.transform([joined_text]).toarray()\n",
    "        \n",
    "        if model_type == \"dnn\":\n",
    "            class DatasetWrapper:\n",
    "                def __init__(self, X):\n",
    "                    self.X = X\n",
    "            probability = model.predict(DatasetWrapper(X_new))[0][0]\n",
    "        else:\n",
    "            probability = model.predict_proba(X_new)[0]\n",
    "\n",
    "    prediction = \"AI\" if probability >= 0.5 else \"Human\"\n",
    "    return prediction, probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on 30 samples...\n",
      "Progress: 3.3% (1/30) - Elapsed: 0.0s\n",
      "Progress: 6.7% (2/30) - Elapsed: 0.0s\n",
      "Progress: 10.0% (3/30) - Elapsed: 0.0s\n",
      "Progress: 13.3% (4/30) - Elapsed: 0.0s\n",
      "Progress: 16.7% (5/30) - Elapsed: 0.0s\n",
      "Progress: 20.0% (6/30) - Elapsed: 0.0s\n",
      "Progress: 23.3% (7/30) - Elapsed: 0.1s\n",
      "Progress: 26.7% (8/30) - Elapsed: 0.1s\n",
      "Progress: 30.0% (9/30) - Elapsed: 0.1s\n",
      "Progress: 33.3% (10/30) - Elapsed: 0.1s\n",
      "Progress: 36.7% (11/30) - Elapsed: 0.1s\n",
      "Progress: 40.0% (12/30) - Elapsed: 0.1s\n",
      "Progress: 43.3% (13/30) - Elapsed: 0.1s\n",
      "Progress: 46.7% (14/30) - Elapsed: 0.1s\n",
      "Progress: 50.0% (15/30) - Elapsed: 0.1s\n",
      "Progress: 53.3% (16/30) - Elapsed: 0.1s\n",
      "Progress: 56.7% (17/30) - Elapsed: 0.1s\n",
      "Progress: 60.0% (18/30) - Elapsed: 0.1s\n",
      "Progress: 63.3% (19/30) - Elapsed: 0.1s\n",
      "Progress: 66.7% (20/30) - Elapsed: 0.1s\n",
      "Progress: 70.0% (21/30) - Elapsed: 0.1s\n",
      "Progress: 73.3% (22/30) - Elapsed: 0.1s\n",
      "Progress: 76.7% (23/30) - Elapsed: 0.1s\n",
      "Progress: 80.0% (24/30) - Elapsed: 0.1s\n",
      "Progress: 83.3% (25/30) - Elapsed: 0.1s\n",
      "Progress: 86.7% (26/30) - Elapsed: 0.1s\n",
      "Progress: 90.0% (27/30) - Elapsed: 0.1s\n",
      "Progress: 93.3% (28/30) - Elapsed: 0.1s\n",
      "Progress: 96.7% (29/30) - Elapsed: 0.1s\n",
      "Progress: 100.0% (30/30) - Elapsed: 0.2s\n",
      "Predictions completed in 0.2 seconds\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model_type in models.keys():\n",
    "    results[model_type] = pd.DataFrame(columns=['ID', 'Label'])\n",
    "\n",
    "total_rows = len(inputs_df)\n",
    "print(f\"Making predictions on {total_rows} samples...\")\n",
    "\n",
    "start_time = time.time()\n",
    "update_interval = max(1, total_rows // 20)\n",
    "\n",
    "for index, row in inputs_df.iterrows():\n",
    "    if index % update_interval == 0 or index == total_rows - 1:\n",
    "        elapsed = time.time() - start_time\n",
    "        progress = (index + 1) / total_rows * 100\n",
    "        print(f\"Progress: {progress:.1f}% ({index+1}/{total_rows}) - Elapsed: {elapsed:.1f}s\")\n",
    "    \n",
    "    id_val = row['ID']\n",
    "    text = row['Text']\n",
    "    \n",
    "    for model_type in models.keys():\n",
    "        try:\n",
    "            model = models[model_type]\n",
    "            vectorizer = vectorizers[model_type]\n",
    "            word_to_idx = word_to_idxs[model_type]\n",
    "            embedding_matrix = embedding_matrices[model_type]\n",
    "            \n",
    "            prediction, _ = predict_text(model, vectorizer, word_to_idx, embedding_matrix, text, model_type)\n",
    "            new_row = pd.DataFrame({'ID': [id_val], 'Label': [prediction]})\n",
    "            results[model_type] = pd.concat([results[model_type], new_row], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_type} model on text {id_val}: {str(e)}\")\n",
    "            new_row = pd.DataFrame({'ID': [id_val], 'Label': ['Error']})\n",
    "            results[model_type] = pd.concat([results[model_type], new_row], ignore_index=True)\n",
    "\n",
    "print(f\"Predictions completed in {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions for logistic model to results/logistic_predictions.csv\n",
      "Saved predictions for dnn model to results/dnn_predictions.csv\n",
      "Saved predictions for rnn model to results/rnn_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "for model_type, df in results.items():\n",
    "    output_path = f\"results/{model_type}_predictions.csv\"\n",
    "    df.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"Saved predictions for {model_type} model to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After splitting 'ID' column:\n",
      "Output dataset columns: ['ID', 'Label']\n",
      "\n",
      "Model Performance:\n",
      "--------------------------------------------------\n",
      "LOGISTIC Model:\n",
      "  Accuracy: 0.5000 (15/30)\n",
      "  Percentage: 50.00%\n",
      "--------------------------------------------------\n",
      "DNN Model:\n",
      "  Accuracy: 0.4667 (14/30)\n",
      "  Percentage: 46.67%\n",
      "--------------------------------------------------\n",
      "RNN Model:\n",
      "  Accuracy: 0.5667 (17/30)\n",
      "  Percentage: 56.67%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "outputs_df[['ID', 'Label']] = outputs_df['ID'].str.split('\\t', expand=True)\n",
    "\n",
    "print(\"\\nAfter splitting 'ID' column:\")\n",
    "print(f\"Output dataset columns: {outputs_df.columns.tolist()}\")\n",
    "\n",
    "def calculate_accuracy(predictions_df, ground_truth_df):\n",
    "    try:\n",
    "        for df, name in [(predictions_df, 'predictions'), (ground_truth_df, 'ground truth')]:\n",
    "            if 'ID' not in df.columns:\n",
    "                raise KeyError(f\"'ID' column not found in {name} dataframe\")\n",
    "            if 'Label' not in df.columns:\n",
    "                raise KeyError(f\"'Label' column not found in {name} dataframe\")\n",
    "        \n",
    "        merged_df = predictions_df.merge(ground_truth_df, on='ID', suffixes=('_pred', '_true'))\n",
    "        \n",
    "        correct = (merged_df['Label_pred'] == merged_df['Label_true']).sum()\n",
    "        total = len(merged_df)\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        return accuracy, correct, total\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating accuracy: {str(e)}\")\n",
    "        print(f\"Predictions columns: {predictions_df.columns.tolist()}\")\n",
    "        print(f\"Ground truth columns: {ground_truth_df.columns.tolist()}\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(\"-\" * 50)\n",
    "for model_type, df in results.items():\n",
    "    accuracy, correct, total = calculate_accuracy(df, outputs_df)\n",
    "    print(f\"{model_type.upper()} Model:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "    print(f\"  Percentage: {accuracy * 100:.2f}%\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Agreement Analysis:\n",
      "Total samples: 30\n",
      "All models agree: 15 (50.00%)\n",
      "All models correct: 8 (26.67%)\n",
      "\n",
      "Samples where models disagree: 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>True_Label</th>\n",
       "      <th>logistic_pred</th>\n",
       "      <th>dnn_pred</th>\n",
       "      <th>rnn_pred</th>\n",
       "      <th>all_agree</th>\n",
       "      <th>all_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1-3</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1-4</td>\n",
       "      <td>AI</td>\n",
       "      <td>Human</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1-5</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D1-8</td>\n",
       "      <td>AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D1-10</td>\n",
       "      <td>AI</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D1-11</td>\n",
       "      <td>Human</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D1-14</td>\n",
       "      <td>AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D1-16</td>\n",
       "      <td>AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D1-19</td>\n",
       "      <td>Human</td>\n",
       "      <td>Human</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D1-20</td>\n",
       "      <td>AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>Human</td>\n",
       "      <td>AI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID True_Label logistic_pred dnn_pred rnn_pred  all_agree  all_correct\n",
       "2    D1-3      Human            AI    Human       AI      False        False\n",
       "3    D1-4         AI         Human    Human       AI      False        False\n",
       "4    D1-5      Human            AI    Human       AI      False        False\n",
       "7    D1-8         AI            AI    Human       AI      False        False\n",
       "9   D1-10         AI         Human       AI       AI      False        False\n",
       "10  D1-11      Human         Human       AI       AI      False        False\n",
       "13  D1-14         AI            AI    Human       AI      False        False\n",
       "15  D1-16         AI            AI    Human       AI      False        False\n",
       "18  D1-19      Human         Human    Human       AI      False        False\n",
       "19  D1-20         AI            AI    Human       AI      False        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(results) > 1:\n",
    "    try:\n",
    "        combined_df = outputs_df[['ID', 'Label']].rename(columns={'Label': 'True_Label'})\n",
    "        \n",
    "        for model_type, df in results.items():\n",
    "            combined_df = combined_df.merge(df, on='ID')\n",
    "            combined_df = combined_df.rename(columns={'Label': f'{model_type}_pred'})\n",
    "        \n",
    "        pred_columns = [col for col in combined_df.columns if col.endswith('_pred')]\n",
    "        combined_df['all_agree'] = combined_df[pred_columns].apply(lambda row: len(set(row)) == 1, axis=1)\n",
    "        combined_df['all_correct'] = combined_df.apply(\n",
    "            lambda row: row['all_agree'] and row[pred_columns[0]] == row['True_Label'], \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        total = len(combined_df)\n",
    "        agree_count = combined_df['all_agree'].sum()\n",
    "        all_correct_count = combined_df['all_correct'].sum()\n",
    "        \n",
    "        print(\"\\nModel Agreement Analysis:\")\n",
    "        print(f\"Total samples: {total}\")\n",
    "        print(f\"All models agree: {agree_count} ({agree_count/total*100:.2f}%)\")\n",
    "        print(f\"All models correct: {all_correct_count} ({all_correct_count/total*100:.2f}%)\")\n",
    "        \n",
    "        disagreement_df = combined_df[~combined_df['all_agree']]\n",
    "        print(f\"\\nSamples where models disagree: {len(disagreement_df)}\")\n",
    "        display(disagreement_df.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in model agreement analysis: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
